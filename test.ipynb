{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score comments on test dataset\n",
    "def removeEmojis(text):\n",
    "    pattern = re.compile(\n",
    "        pattern = \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"]+\", flags = re.UNICODE)\n",
    "    return pattern.sub(r'', str(text))\n",
    "\n",
    "def hasOnlyLatinCharsOrArabicNumerals(text):\n",
    "    try:\n",
    "        text.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return ''\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "filepath = 'datasets/YT API/mrkAmmMakMg_15323.csv'\n",
    "filename = re.findall(r'[^/]+(?=.csv)', filepath)[0]\n",
    "testdf = pd.read_csv(filepath)\n",
    "# remove emojis\n",
    "testdf['comment'] = testdf['comment'].apply(lambda s: removeEmojis(s))\n",
    "# remove comments with non-latin alphabets or arabic numerals\n",
    "testdf['comment'] = testdf['comment'].apply(lambda s: hasOnlyLatinCharsOrArabicNumerals(s))\n",
    "# rempty comments and duplicates\n",
    "testdf = testdf.replace('', float('NaN')).dropna()\n",
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "progress = 0\n",
    "length = len(testdf)\n",
    "\n",
    "def pre_process(text):\n",
    "    global progress\n",
    "    progress += 1\n",
    "    '''\n",
    "    - transform to lowercase\n",
    "    - remove links and mentions\n",
    "    - remove all characters except whitespaces and latin characters\n",
    "    - word tokenization\n",
    "    - lemmatization\n",
    "    '''\n",
    "    removed_mentions = re.sub(r'\\s?@(\\s)*(\\S*)\\s?', ' ', text)\n",
    "    removed_links = re.sub(r'((http|watch\\?v=|[wW]{3})\\S+)', ' ', removed_mentions)\n",
    "    normalized = re.sub(r'[^A-Za-z]+', ' ', removed_links)\n",
    "    tokens = word_tokenize(normalized)\n",
    "    tokens = [word for word in tokens if not word in stopwords.words('english')]\n",
    "\n",
    "    print(f'{progress / length * 100:.2f}%\\t{progress}/{length}', end='\\r')\n",
    "    return ' '.join(tokens).lower()\n",
    "\n",
    "model = load_model('spam_detector')\n",
    "testdf['score'] = testdf['comment'].apply(lambda s: round(model.predict([pre_process(s)], verbose=0)[0][0] * 100, 2))\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments with 90% score or more is spam\n",
    "testdf[testdf['score'] >= 90].sort_values(by='score', ascending=False).to_csv(f'spam/{filename}.csv', index=False)\n",
    "testdf[testdf['score'] >= 90].sort_values(by='score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fd5f0ae0ac350a363f5a11ffbf7b24bda9f74a3f3d508a6abe7beb4472e3c34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
